#!/usr/bin/env python3
"""
Lightweight throughput benchmarking harness for Lean SDK.

Measures how many Lean files can be compiled per unit time on a device,
with and without mathlib/batteries dependencies.
"""

import argparse
import json
import random
import shutil
import string
import tempfile
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from lean_bench.compiler import (
    CompilerOutput,
    compile_lean_content,
    compile_lean_file,
)


@dataclass
class BenchmarkConfig:
    """Configuration for benchmark runs."""
    
    # File generation parameters
    theorems_per_file: List[int] = field(default_factory=lambda: [10, 50, 100, 500])
    num_files: int = 20
    
    # Compilation parameters
    parallel_workers: int = 4
    timeout_seconds: int = 60
    warmup_files: int = 5
    
    # Test scenarios
    test_with_mathlib: bool = True
    test_without_mathlib: bool = True
    
    # Output settings
    verbose: bool = False
    output_json: bool = True


@dataclass
class BenchmarkResult:
    """Results from a single benchmark run."""
    
    scenario: str  # e.g., "no_mathlib_100_theorems"
    num_files: int
    theorems_per_file: int
    uses_mathlib: bool
    
    # Timing metrics
    total_time_seconds: float
    compilation_times: List[float]
    successful_compilations: int
    failed_compilations: int
    
    # Throughput metrics
    files_per_second: float
    files_per_minute: float
    theorems_per_second: float
    
    # Statistics
    avg_time_per_file: float
    min_time: float
    max_time: float
    median_time: float
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "scenario": self.scenario,
            "num_files": self.num_files,
            "theorems_per_file": self.theorems_per_file,
            "uses_mathlib": self.uses_mathlib,
            "total_time_seconds": round(self.total_time_seconds, 3),
            "successful_compilations": self.successful_compilations,
            "failed_compilations": self.failed_compilations,
            "files_per_second": round(self.files_per_second, 3),
            "files_per_minute": round(self.files_per_minute, 1),
            "theorems_per_second": round(self.theorems_per_second, 2),
            "avg_time_per_file": round(self.avg_time_per_file, 3),
            "min_time": round(self.min_time, 3),
            "max_time": round(self.max_time, 3),
            "median_time": round(self.median_time, 3),
        }


class LeanFileGenerator:
    """Generate simple Lean files with autogenerated theorem/function names."""
    
    @staticmethod
    def generate_random_name(prefix: str = "theorem") -> str:
        """Generate a random unique name for theorems/functions."""
        suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))
        return f"{prefix}_{suffix}"
    
    @staticmethod
    def generate_simple_theorem() -> str:
        """Generate a simple theorem with autogenerated name."""
        name = LeanFileGenerator.generate_random_name("theorem")
        return f"""theorem {name} (n : Nat) : n + 0 = n := by
  simp"""
    
    @staticmethod
    def generate_simple_function() -> str:
        """Generate a simple function with autogenerated name."""
        name = LeanFileGenerator.generate_random_name("func")
        return f"""def {name} (n : Nat) : Nat :=
  n + 1"""
    
    @staticmethod
    def generate_simple_lemma() -> str:
        """Generate a simple lemma with autogenerated name."""
        name = LeanFileGenerator.generate_random_name("thm")
        # Use theorem instead of lemma for better compatibility
        return f"""theorem {name} (a b : Nat) : a + b = b + a := by
  rw [Nat.add_comm]"""
    
    @staticmethod
    def generate_file_content(
        num_theorems: int,
        use_mathlib: bool = False,
        mix_types: bool = True
    ) -> str:
        """
        Generate a Lean file with the specified number of theorems/functions.
        
        Args:
            num_theorems: Number of theorems/functions to generate
            use_mathlib: Whether to include mathlib imports
            mix_types: Whether to mix theorems, functions, and lemmas
        
        Returns:
            Complete Lean file content as string
        """
        lines = []
        
        # Add imports if using mathlib
        if use_mathlib:
            lines.append("import Mathlib.Data.Nat.Basic")
            lines.append("import Mathlib.Tactic")
            lines.append("")
        
        # Add file header comment
        lines.append(f"-- Auto-generated benchmark file with {num_theorems} definitions")
        lines.append("")
        
        # Generate theorems/functions
        generators = [
            LeanFileGenerator.generate_simple_theorem,
            LeanFileGenerator.generate_simple_function,
            LeanFileGenerator.generate_simple_lemma,
        ]
        
        for i in range(num_theorems):
            if mix_types:
                generator = random.choice(generators)
            else:
                generator = generators[0]  # Only theorems
            
            lines.append(generator())
            lines.append("")  # Empty line between definitions
        
        return "\n".join(lines)


class ThroughputBenchmark:
    """Main benchmarking harness for measuring Lean compilation throughput."""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.temp_dir: Optional[Path] = None
        self.results: List[BenchmarkResult] = []
    
    def setup_workspace(self, use_mathlib: bool) -> Path:
        """Set up a temporary Lean workspace for benchmarking."""
        # Create temporary directory
        temp_dir = Path(tempfile.mkdtemp(prefix="lean_benchmark_"))
        
        # Set up Lean 4 project with lakefile
        print(f"Setting up Lean 4 project (mathlib={use_mathlib})...")
        
        # Create lakefile.lean
        lakefile_content = f'''import Lake
open Lake DSL

package «benchmark» where
  -- add package configuration options here

lean_lib «Benchmark» where
  -- add library configuration options here
'''
        
        if use_mathlib:
            lakefile_content = f'''import Lake
open Lake DSL

package «benchmark» where
  -- add package configuration options here

require mathlib from git
  "https://github.com/leanprover-community/mathlib4.git"

@[default_target]
lean_lib «Benchmark» where
  -- add library configuration options here
'''
        
        lakefile_path = temp_dir / "lakefile.lean"
        lakefile_path.write_text(lakefile_content)
        
        # Create Benchmark.lean file (Lean 4 requires a main lib file)
        benchmark_lib = temp_dir / "Benchmark.lean"
        benchmark_lib.write_text("-- Benchmark library\n")
        
        # Create Benchmark directory for additional files
        src_dir = temp_dir / "Benchmark"
        src_dir.mkdir(exist_ok=True)
        
        return temp_dir
    
    def generate_test_files(
        self,
        workspace: Path,
        num_files: int,
        theorems_per_file: int,
        use_mathlib: bool
    ) -> List[Path]:
        """Generate test Lean files in the workspace."""
        # For Lean 4, put files in Benchmark directory
        src_dir = workspace / "Benchmark"
        file_paths = []
        
        for i in range(num_files):
            content = LeanFileGenerator.generate_file_content(
                theorems_per_file,
                use_mathlib=use_mathlib
            )
            
            file_name = f"Benchmark{i:04d}.lean"
            file_path = src_dir / file_name
            file_path.write_text(content)
            file_paths.append(file_path)
        
        return file_paths
    
    def compile_file_timed(self, file_path: Path, workspace: Path) -> Tuple[float, bool]:
        """
        Compile a single file and return timing information.
        
        Returns:
            Tuple of (compilation_time_seconds, success)
        """
        start_time = time.time()
        result = compile_lean_file(
            file_path,
            workspace,
            timeout=self.config.timeout_seconds
        )
        elapsed = time.time() - start_time
        
        return elapsed, result.success
    
    def run_benchmark_scenario(
        self,
        theorems_per_file: int,
        use_mathlib: bool,
        parallel: bool = False
    ) -> BenchmarkResult:
        """Run a single benchmark scenario."""
        scenario_name = f"{'mathlib' if use_mathlib else 'no_mathlib'}_{theorems_per_file}_theorems"
        
        print(f"\n{'='*60}")
        print(f"Running scenario: {scenario_name}")
        print(f"  Files: {self.config.num_files}")
        print(f"  Theorems per file: {theorems_per_file}")
        print(f"  Mathlib: {use_mathlib}")
        print(f"  Parallel: {parallel}")
        print(f"{'='*60}")
        
        # Set up workspace
        workspace = self.setup_workspace(use_mathlib)
        
        try:
            # Generate test files
            print(f"Generating {self.config.num_files} test files...")
            file_paths = self.generate_test_files(
                workspace,
                self.config.num_files,
                theorems_per_file,
                use_mathlib
            )
            
            # Warmup runs
            if self.config.warmup_files > 0:
                print(f"Running {self.config.warmup_files} warmup compilations...")
                for i in range(min(self.config.warmup_files, len(file_paths))):
                    self.compile_file_timed(file_paths[i], workspace)
            
            # Main benchmark run
            print(f"Starting benchmark run...")
            compilation_times = []
            successful = 0
            failed = 0
            
            start_time = time.time()
            
            if parallel and self.config.parallel_workers > 1:
                # Parallel compilation
                with ProcessPoolExecutor(max_workers=self.config.parallel_workers) as executor:
                    futures = {
                        executor.submit(self.compile_file_timed, fp, workspace): fp
                        for fp in file_paths
                    }
                    
                    for future in as_completed(futures):
                        elapsed, success = future.result()
                        compilation_times.append(elapsed)
                        if success:
                            successful += 1
                        else:
                            failed += 1
                        
                        if self.config.verbose:
                            status = "✓" if success else "✗"
                            print(f"  {status} Compiled in {elapsed:.3f}s")
            else:
                # Sequential compilation
                for i, file_path in enumerate(file_paths):
                    elapsed, success = self.compile_file_timed(file_path, workspace)
                    compilation_times.append(elapsed)
                    
                    if success:
                        successful += 1
                    else:
                        failed += 1
                    
                    if self.config.verbose:
                        status = "✓" if success else "✗"
                        print(f"  [{i+1}/{len(file_paths)}] {status} {elapsed:.3f}s")
                    elif (i + 1) % 5 == 0:
                        print(f"  Progress: {i+1}/{len(file_paths)} files compiled")
            
            total_time = time.time() - start_time
            
            # Calculate metrics
            files_per_second = len(file_paths) / total_time if total_time > 0 else 0
            files_per_minute = files_per_second * 60
            theorems_per_second = (len(file_paths) * theorems_per_file) / total_time if total_time > 0 else 0
            
            # Calculate statistics
            sorted_times = sorted(compilation_times)
            avg_time = sum(compilation_times) / len(compilation_times) if compilation_times else 0
            min_time = min(compilation_times) if compilation_times else 0
            max_time = max(compilation_times) if compilation_times else 0
            median_time = sorted_times[len(sorted_times) // 2] if sorted_times else 0
            
            result = BenchmarkResult(
                scenario=scenario_name,
                num_files=len(file_paths),
                theorems_per_file=theorems_per_file,
                uses_mathlib=use_mathlib,
                total_time_seconds=total_time,
                compilation_times=compilation_times,
                successful_compilations=successful,
                failed_compilations=failed,
                files_per_second=files_per_second,
                files_per_minute=files_per_minute,
                theorems_per_second=theorems_per_second,
                avg_time_per_file=avg_time,
                min_time=min_time,
                max_time=max_time,
                median_time=median_time
            )
            
            # Print summary
            print(f"\n{'-'*40}")
            print(f"Scenario Results: {scenario_name}")
            print(f"{'-'*40}")
            print(f"Total time: {total_time:.2f} seconds")
            print(f"Files compiled: {successful}/{len(file_paths)} successful")
            print(f"Throughput:")
            print(f"  - {files_per_second:.2f} files/second")
            print(f"  - {files_per_minute:.1f} files/minute")
            print(f"  - {theorems_per_second:.1f} theorems/second")
            print(f"Per-file statistics:")
            print(f"  - Average: {avg_time:.3f}s")
            print(f"  - Min: {min_time:.3f}s")
            print(f"  - Max: {max_time:.3f}s")
            print(f"  - Median: {median_time:.3f}s")
            
            return result
            
        finally:
            # Cleanup workspace
            if workspace.exists():
                shutil.rmtree(workspace, ignore_errors=True)
    
    def run_all_benchmarks(self) -> List[BenchmarkResult]:
        """Run all configured benchmark scenarios."""
        results = []
        
        scenarios = []
        
        # Build list of scenarios to run
        for theorems in self.config.theorems_per_file:
            if self.config.test_without_mathlib:
                scenarios.append((theorems, False))
            if self.config.test_with_mathlib:
                scenarios.append((theorems, True))
        
        total_scenarios = len(scenarios)
        
        print(f"\nRunning {total_scenarios} benchmark scenarios...")
        print(f"Configuration:")
        print(f"  - Files per scenario: {self.config.num_files}")
        print(f"  - Theorem counts: {self.config.theorems_per_file}")
        print(f"  - Test with mathlib: {self.config.test_with_mathlib}")
        print(f"  - Test without mathlib: {self.config.test_without_mathlib}")
        print(f"  - Parallel workers: {self.config.parallel_workers}")
        
        # Run each scenario
        for i, (theorems, use_mathlib) in enumerate(scenarios, 1):
            print(f"\n[{i}/{total_scenarios}] ", end="")
            
            # Test both sequential and parallel if workers > 1
            if self.config.parallel_workers > 1:
                # Run parallel version
                result = self.run_benchmark_scenario(
                    theorems,
                    use_mathlib,
                    parallel=True
                )
                results.append(result)
            else:
                # Run sequential version only
                result = self.run_benchmark_scenario(
                    theorems,
                    use_mathlib,
                    parallel=False
                )
                results.append(result)
        
        self.results = results
        return results
    
    def print_summary_report(self):
        """Print a summary report of all benchmark results."""
        if not self.results:
            print("No benchmark results to report.")
            return
        
        print(f"\n{'='*60}")
        print("BENCHMARK SUMMARY REPORT")
        print(f"{'='*60}")
        
        # Group results by mathlib usage
        with_mathlib = [r for r in self.results if r.uses_mathlib]
        without_mathlib = [r for r in self.results if not r.uses_mathlib]
        
        def print_group_summary(results: List[BenchmarkResult], title: str):
            if not results:
                return
            
            print(f"\n{title}:")
            print(f"{'-'*40}")
            
            for r in results:
                print(f"\n{r.theorems_per_file} theorems/file:")
                print(f"  Throughput: {r.files_per_second:.2f} files/s ({r.files_per_minute:.0f} files/min)")
                print(f"  Success rate: {r.successful_compilations}/{r.num_files} ({r.successful_compilations/r.num_files*100:.1f}%)")
                print(f"  Avg time/file: {r.avg_time_per_file:.3f}s")
        
        print_group_summary(without_mathlib, "WITHOUT MATHLIB")
        print_group_summary(with_mathlib, "WITH MATHLIB")
        
        # Overall statistics
        print(f"\n{'='*60}")
        print("OVERALL STATISTICS")
        print(f"{'='*60}")
        
        all_throughputs = [r.files_per_second for r in self.results]
        if all_throughputs:
            print(f"Best throughput: {max(all_throughputs):.2f} files/second")
            print(f"Worst throughput: {min(all_throughputs):.2f} files/second")
            print(f"Average throughput: {sum(all_throughputs)/len(all_throughputs):.2f} files/second")
        
        # Performance comparison
        if with_mathlib and without_mathlib:
            print(f"\n{'='*60}")
            print("MATHLIB OVERHEAD ANALYSIS")
            print(f"{'='*60}")
            
            for theorems in self.config.theorems_per_file:
                with_m = next((r for r in with_mathlib if r.theorems_per_file == theorems), None)
                without_m = next((r for r in without_mathlib if r.theorems_per_file == theorems), None)
                
                if with_m and without_m:
                    overhead = (without_m.files_per_second - with_m.files_per_second) / without_m.files_per_second * 100
                    print(f"\n{theorems} theorems/file:")
                    print(f"  Without mathlib: {without_m.files_per_second:.2f} files/s")
                    print(f"  With mathlib: {with_m.files_per_second:.2f} files/s")
                    print(f"  Overhead: {overhead:.1f}% slower with mathlib")
    
    def save_results_json(self, output_path: Path):
        """Save benchmark results to JSON file."""
        data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "config": {
                "theorems_per_file": self.config.theorems_per_file,
                "num_files": self.config.num_files,
                "parallel_workers": self.config.parallel_workers,
                "timeout_seconds": self.config.timeout_seconds,
            },
            "results": [r.to_dict() for r in self.results],
        }
        
        with open(output_path, "w") as f:
            json.dump(data, f, indent=2)
        
        print(f"\nResults saved to: {output_path}")


def main():
    """Main entry point for the benchmark script."""
    parser = argparse.ArgumentParser(
        description="Lean SDK throughput benchmarking tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run default benchmarks
  python benchmark_throughput.py
  
  # Test specific theorem counts
  python benchmark_throughput.py --theorems 10 50 100
  
  # Test with more files
  python benchmark_throughput.py --num-files 50
  
  # Test only without mathlib
  python benchmark_throughput.py --no-mathlib-only
  
  # Use parallel compilation
  python benchmark_throughput.py --parallel 8
  
  # Save results to custom location
  python benchmark_throughput.py --output results.json
        """
    )
    
    parser.add_argument(
        "--theorems",
        nargs="+",
        type=int,
        default=[10, 50, 100, 500],
        help="Number of theorems per file to test (default: 10 50 100 500)"
    )
    
    parser.add_argument(
        "--num-files",
        type=int,
        default=20,
        help="Number of files to compile per scenario (default: 20)"
    )
    
    parser.add_argument(
        "--parallel",
        type=int,
        default=1,
        help="Number of parallel workers for compilation (default: 1)"
    )
    
    parser.add_argument(
        "--timeout",
        type=int,
        default=60,
        help="Timeout per file compilation in seconds (default: 60)"
    )
    
    parser.add_argument(
        "--warmup",
        type=int,
        default=5,
        help="Number of warmup compilations (default: 5)"
    )
    
    parser.add_argument(
        "--no-mathlib-only",
        action="store_true",
        help="Only test without mathlib"
    )
    
    parser.add_argument(
        "--mathlib-only",
        action="store_true",
        help="Only test with mathlib"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Show detailed compilation progress"
    )
    
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("benchmark_results.json"),
        help="Output JSON file path (default: benchmark_results.json)"
    )
    
    parser.add_argument(
        "--no-json",
        action="store_true",
        help="Don't save JSON output"
    )
    
    args = parser.parse_args()
    
    # Configure benchmark
    config = BenchmarkConfig(
        theorems_per_file=args.theorems,
        num_files=args.num_files,
        parallel_workers=args.parallel,
        timeout_seconds=args.timeout,
        warmup_files=args.warmup,
        test_with_mathlib=not args.no_mathlib_only,
        test_without_mathlib=not args.mathlib_only,
        verbose=args.verbose,
        output_json=not args.no_json
    )
    
    # Run benchmarks
    print("Lean SDK Throughput Benchmark")
    print("="*60)
    
    benchmark = ThroughputBenchmark(config)
    
    try:
        results = benchmark.run_all_benchmarks()
        
        # Print summary
        benchmark.print_summary_report()
        
        # Save results
        if config.output_json:
            benchmark.save_results_json(args.output)
        
        print("\nBenchmark completed successfully!")
        
    except KeyboardInterrupt:
        print("\n\nBenchmark interrupted by user.")
        return 1
    except Exception as e:
        print(f"\nError during benchmark: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
